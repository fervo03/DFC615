{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DFC615_run_MRC.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fervo03/DFC615/blob/main/DFC615_run_MRC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9S-RkQ6cx4II"
      },
      "source": [
        "# [DFC615_project] KoELECTRA 를 활용한 MRC 구현\n",
        "# 2020517001_이종열\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OaUcXO3zUqrD",
        "outputId": "fb98c0bc-7d2c-4852-8967-6c7319839392"
      },
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "data_dir = '/content/gdrive/My Drive/Colab Notebooks/DFC615'\n",
        "output_dir = '/content/gdrive/My Drive/Colab Notebooks/DFC615/output'\n",
        "os.chdir(data_dir)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tYiR35n5Uc2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1035344-514c-45e2-c2e5-57ed587c6146"
      },
      "source": [
        "!pip install transformers==3.3.1\n",
        "!pip install seqeval\n",
        "!pip install fastprogress\n",
        "!pip install attrdict\n",
        "!pip install pandas\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "!pip install konlpy  # 추가"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers==3.3.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/22/aff234f4a841f8999e68a7a94bdd4b60b4cebcfeca5d67d61cd08c9179de/transformers-3.3.1-py3-none-any.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 7.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.1) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.1) (4.41.1)\n",
            "Collecting tokenizers==0.8.1.rc2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/26/c02ba92ecb8b780bdae4a862d351433c2912fe49469dac7f87a5c85ccca6/tokenizers-0.8.1rc2-cp37-cp37m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 29.4MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 32.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.1) (20.9)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/aa/1437691b0c7c83086ebb79ce2da16e00bef024f24fec2a5161c35476f499/sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 39.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.1) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.1) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.1) (2.23.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.3.1) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.3.1) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.3.1) (1.0.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.3.1) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.3.1) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.3.1) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.3.1) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.3.1) (2021.5.30)\n",
            "Installing collected packages: tokenizers, sacremoses, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.45 sentencepiece-0.1.96 tokenizers-0.8.1rc2 transformers-3.3.1\n",
            "Collecting seqeval\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9d/2d/233c79d5b4e5ab1dbf111242299153f3caddddbb691219f363ad55ce783d/seqeval-1.2.2.tar.gz (43kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 4.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.0.1)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-cp37-none-any.whl size=16184 sha256=afa95966954b1a1be1e0eea215f0c924f2656308ba48096b503b9fea2c35bb54\n",
            "  Stored in directory: /root/.cache/pip/wheels/52/df/1b/45d75646c37428f7e626214704a0e35bd3cfc32eda37e59e5f\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-1.2.2\n",
            "Requirement already satisfied: fastprogress in /usr/local/lib/python3.7/dist-packages (1.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fastprogress) (1.19.5)\n",
            "Collecting attrdict\n",
            "  Downloading https://files.pythonhosted.org/packages/ef/97/28fe7e68bc7adfce67d4339756e85e9fcf3c6fd7f0c0781695352b70472c/attrdict-2.0.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from attrdict) (1.15.0)\n",
            "Installing collected packages: attrdict\n",
            "Successfully installed attrdict-2.0.1\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.19.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Collecting konlpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4MB 1.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Collecting beautifulsoup4==4.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 10.1MB/s \n",
            "\u001b[?25hCollecting JPype1>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/88/f817ef1af6f794e8f11313dcd1549de833f4599abcec82746ab5ed086686/JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 33.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Installing collected packages: beautifulsoup4, JPype1, colorama, konlpy\n",
            "  Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed JPype1-1.3.0 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KoFBazD43g2a"
      },
      "source": [
        "from __future__ import print_function\n",
        "from collections import Counter\n",
        "import sys, os\n",
        "import argparse\n",
        "import string\n",
        "import re\n",
        "import glob\n",
        "import logging\n",
        "import random\n",
        "import timeit\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from fastprogress.fastprogress import master_bar, progress_bar\n",
        "from attrdict import AttrDict\n",
        "from konlpy.tag import Hannanum  # 추가\n",
        "\n",
        "from transformers import (\n",
        "    AdamW,\n",
        "    get_linear_schedule_with_warmup,\n",
        "    squad_convert_examples_to_features\n",
        ")\n",
        "\n",
        "from transformers.data.metrics.squad_metrics import (\n",
        "    compute_predictions_logits,\n",
        "    squad_evaluate,\n",
        ")\n",
        "\n",
        "from transformers import (\n",
        "    ElectraForQuestionAnswering,\n",
        "    XLMRobertaForQuestionAnswering,  \n",
        "    ElectraTokenizer,\n",
        "    XLMRobertaTokenizer,\n",
        "    ElectraConfig,\n",
        "    XLMRobertaConfig\n",
        ")\n",
        "\n",
        "from transformers.data.processors.squad import SquadResult, SquadV1Processor, SquadV2Processor\n",
        "from transformers import pipeline\n",
        "logger = logging.getLogger(__name__)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKT2VDo1F_f-"
      },
      "source": [
        "def init_logger():\n",
        "    logging.basicConfig(\n",
        "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "        level=logging.INFO,\n",
        "    )\n",
        "\n",
        "def set_seed(args):\n",
        "    random.seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    if not args.no_cuda and torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(args.seed)\n",
        "\n",
        "MODEL_FOR_QUESTION_ANSWERING = {\n",
        "    \"koelectra-base-v3\": ElectraForQuestionAnswering,\n",
        "    \"koelectra-small-v3\": ElectraForQuestionAnswering,\n",
        "}\n",
        "TOKENIZER_CLASSES = {\n",
        "    \"koelectra-base-v3\": ElectraTokenizer,\n",
        "    \"koelectra-small-v3\": ElectraTokenizer,\n",
        "}\n",
        "CONFIG_CLASSES = {\n",
        "    \"koelectra-base-v3\": ElectraConfig,\n",
        "    \"koelectra-small-v3\": ElectraConfig,\n",
        "}"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSY6geyMGDtN"
      },
      "source": [
        "'''KorQuAD v1.0에 대한 공식 평가 스크립트 '''\n",
        "'''본 스크립트는 SQuAD v1.1 평가 스크립트 https://rajpurkar.github.io/SQuAD-explorer/ 를 바탕으로 작성됨.'''\n",
        "\n",
        "def normalize_answer(s):\n",
        "    \n",
        "    tagger = Hannanum()\n",
        "\n",
        "    def remove_(text):\n",
        "\n",
        "        text = re.sub(\"'\", \"\", text)\n",
        "        text = re.sub('\"', \"\", text)\n",
        "        text = re.sub('《', \"\", text)\n",
        "        text = re.sub('》', \"\", text)\n",
        "        text = re.sub('<', \"\", text)\n",
        "        text = re.sub('>', \"\", text)\n",
        "        text = re.sub('〈', \"\", text)\n",
        "        text = re.sub('〉', \"\", text)\n",
        "        text = re.sub(\"‘\", \"\", text)\n",
        "        text = re.sub(\"’\", \"\", text)\n",
        "        text = re.sub(r'\\([^\\)^\\(]*\\)?', '', text)\n",
        "        return text\n",
        "\n",
        "    def white_space_fix(text):\n",
        "        return ' '.join(text.split())\n",
        "\n",
        "    def remove_punc(text):\n",
        "        exclude = set(string.punctuation)\n",
        "        return ''.join(ch for ch in text if ch not in exclude)\n",
        "    \n",
        "    def lower(text):\n",
        "        return text.lower()\n",
        "\n",
        "\n",
        "    # nomalize_answer 함수에 조사를 제거하는 함수 추가\n",
        "    \n",
        "    def trim_tag(text):\n",
        "        pos = tagger.pos(text)\n",
        "        subs = text.split()\n",
        "        head = ' '.join(w for w in subs[:-1])\n",
        "        tail = subs[len(subs)-1]\n",
        "        prefix_rules = ('J','E', 'V')        \n",
        "\n",
        "        for i in reversed(range(len(pos))):\n",
        "            if pos[i][1].startswith(prefix_rules):\n",
        "                tail = re.sub(pos[i][0] + '$', \"\", tail)\n",
        "            else:\n",
        "                break;\n",
        "\n",
        "        refined_text = ' '.join([head, tail]).strip()\n",
        "        print(text, pos, refined_text)\n",
        "        return refined_text\n",
        "\n",
        "    return white_space_fix(trim_tag(remove_punc(lower(remove_(s)))))\n",
        "\n",
        "\n",
        "def f1_score(prediction, ground_truth):\n",
        "    prediction_tokens = normalize_answer(prediction).split()\n",
        "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
        "\n",
        "    # F1 by character\n",
        "    prediction_Char = []\n",
        "    for tok in prediction_tokens:\n",
        "        now = [a for a in tok]\n",
        "        prediction_Char.extend(now)\n",
        "\n",
        "    ground_truth_Char = []\n",
        "    for tok in ground_truth_tokens:\n",
        "        now = [a for a in tok]\n",
        "        ground_truth_Char.extend(now)\n",
        "\n",
        "    common = Counter(prediction_Char) & Counter(ground_truth_Char)\n",
        "    num_same = sum(common.values())\n",
        "    if num_same == 0:\n",
        "        return 0\n",
        "\n",
        "    precision = 1.0 * num_same / len(prediction_Char)\n",
        "    recall = 1.0 * num_same / len(ground_truth_Char)\n",
        "    f1 = (2 * precision * recall) / (precision + recall)\n",
        "\n",
        "    return f1\n",
        "\n",
        "\n",
        "def exact_match_score(prediction, ground_truth):\n",
        "    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n",
        "\n",
        "\n",
        "def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n",
        "    scores_for_ground_truths = []\n",
        "    for ground_truth in ground_truths:\n",
        "        score = metric_fn(prediction, ground_truth)\n",
        "        scores_for_ground_truths.append(score)\n",
        "    return max(scores_for_ground_truths)\n",
        "\n",
        "\n",
        "def evaluate(dataset, predictions):\n",
        "    f1 = exact_match = total = 0\n",
        "    for article in dataset:\n",
        "        for paragraph in article['paragraphs']:\n",
        "            for qa in paragraph['qas']:\n",
        "                total += 1\n",
        "                if qa['id'] not in predictions:\n",
        "                    message = 'Unanswered question ' + qa['id'] + \\\n",
        "                              ' will receive score 0.'\n",
        "                    print(message, file=sys.stderr)\n",
        "                    continue\n",
        "                ground_truths = list(map(lambda x: x['text'], qa['answers']))\n",
        "                prediction = predictions[qa['id']]\n",
        "                exact_match += metric_max_over_ground_truths(\n",
        "                    exact_match_score, prediction, ground_truths)\n",
        "                f1 += metric_max_over_ground_truths(\n",
        "                    f1_score, prediction, ground_truths)\n",
        "\n",
        "    exact_match = 100.0 * exact_match / total\n",
        "    f1 = 100.0 * f1 / total\n",
        "    return {'official_exact_match': exact_match, 'official_f1': f1}\n",
        "\n",
        "\n",
        "def eval_during_train(args, step):\n",
        "    expected_version = 'KorQuAD_v1.0'\n",
        "\n",
        "    dataset_file = os.path.join(args.data_dir, args.predict_file)\n",
        "    prediction_file = os.path.join(args.output_dir, 'predictions_{}.json'.format(step))\n",
        "\n",
        "    with open(dataset_file) as dataset_f:\n",
        "        dataset_json = json.load(dataset_f)\n",
        "        read_version = \"_\".join(dataset_json['version'].split(\"_\")[:-1])\n",
        "        if (read_version != expected_version):\n",
        "            print('Evaluation expects ' + expected_version +\n",
        "                  ', but got dataset with ' + read_version,\n",
        "                  file=sys.stderr)\n",
        "        dataset = dataset_json['data']\n",
        "    with open(prediction_file) as prediction_f:\n",
        "        predictions = json.load(prediction_f)\n",
        "\n",
        "    return evaluate(dataset, predictions)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TPTbiaFJKhG"
      },
      "source": [
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def train(args, train_dataset, model, tokenizer):\n",
        "    \"\"\" Train the model \"\"\"\n",
        "    train_sampler = RandomSampler(train_dataset)\n",
        "    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)\n",
        "\n",
        "    if args.max_steps > 0:\n",
        "        t_total = args.max_steps\n",
        "        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n",
        "    else:\n",
        "        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
        "\n",
        "    # Prepare optimizer and schedule (linear warmup and decay)\n",
        "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "    optimizer_grouped_parameters = [\n",
        "        {\n",
        "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "            \"weight_decay\": args.weight_decay,\n",
        "        },\n",
        "        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
        "    ]\n",
        "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer, num_warmup_steps=int(t_total * args.warmup_proportion), num_training_steps=t_total\n",
        "    )\n",
        "\n",
        "    # Check if saved optimizer or scheduler states exist\n",
        "    if os.path.isfile(os.path.join(args.model_name_or_path, \"optimizer.pt\")) and os.path.isfile(\n",
        "            os.path.join(args.model_name_or_path, \"scheduler.pt\")\n",
        "    ):\n",
        "        # Load in optimizer and scheduler states\n",
        "        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"optimizer.pt\")))\n",
        "        scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"scheduler.pt\")))\n",
        "\n",
        "    # Train!\n",
        "    logger.info(\"***** Running training *****\")\n",
        "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
        "    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n",
        "    logger.info(\"  Train batch size per GPU = %d\", args.train_batch_size)\n",
        "    logger.info(\n",
        "        \"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n",
        "        args.train_batch_size\n",
        "        * args.gradient_accumulation_steps)\n",
        "    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n",
        "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
        "\n",
        "    global_step = 1\n",
        "    epochs_trained = 0\n",
        "    steps_trained_in_current_epoch = 0\n",
        "    # Check if continuing training from a checkpoint\n",
        "    if os.path.exists(args.model_name_or_path):\n",
        "        try:\n",
        "            # set global_step to gobal_step of last saved checkpoint from model path\n",
        "            checkpoint_suffix = args.model_name_or_path.split(\"-\")[-1].split(\"/\")[0]\n",
        "            global_step = int(checkpoint_suffix)\n",
        "            epochs_trained = global_step // (len(train_dataloader) // args.gradient_accumulation_steps)\n",
        "            steps_trained_in_current_epoch = global_step % (len(train_dataloader) // args.gradient_accumulation_steps)\n",
        "\n",
        "            logger.info(\"  Continuing training from checkpoint, will skip to saved global_step\")\n",
        "            logger.info(\"  Continuing training from epoch %d\", epochs_trained)\n",
        "            logger.info(\"  Continuing training from global step %d\", global_step)\n",
        "            logger.info(\"  Will skip the first %d steps in the first epoch\", steps_trained_in_current_epoch)\n",
        "        except ValueError:\n",
        "            logger.info(\"  Starting fine-tuning.\")\n",
        "\n",
        "    tr_loss, logging_loss = 0.0, 0.0\n",
        "    model.zero_grad()\n",
        "    mb = master_bar(range(int(args.num_train_epochs)))\n",
        "    # Added here for reproductibility\n",
        "    set_seed(args)\n",
        "\n",
        "    for epoch in mb:\n",
        "        epoch_iterator = progress_bar(train_dataloader, parent=mb)\n",
        "        for step, batch in enumerate(epoch_iterator):\n",
        "            # Skip past any already trained steps if resuming training\n",
        "            if steps_trained_in_current_epoch > 0:\n",
        "                steps_trained_in_current_epoch -= 1\n",
        "                continue\n",
        "\n",
        "            model.train()\n",
        "            batch = tuple(t.to(args.device) for t in batch)\n",
        "\n",
        "            inputs = {\n",
        "                \"input_ids\": batch[0],\n",
        "                \"attention_mask\": batch[1],\n",
        "                \"token_type_ids\": batch[2],\n",
        "                \"start_positions\": batch[3],\n",
        "                \"end_positions\": batch[4],\n",
        "            }\n",
        "\n",
        "            if args.model_type in [\"xlm\", \"roberta\", \"distilbert\", \"distilkobert\", \"xlm-roberta\"]:\n",
        "                del inputs[\"token_type_ids\"]\n",
        "\n",
        "            if args.model_type in [\"xlnet\", \"xlm\"]:\n",
        "                inputs.update({\"cls_index\": batch[5], \"p_mask\": batch[6]})\n",
        "                if args.version_2_with_negative:\n",
        "                    inputs.update({\"is_impossible\": batch[7]})\n",
        "                if hasattr(model, \"config\") and hasattr(model.config, \"lang2id\"):\n",
        "                    inputs.update(\n",
        "                        {\"langs\": (torch.ones(batch[0].shape, dtype=torch.int64) * args.lang_id).to(args.device)}\n",
        "                    )\n",
        "\n",
        "            outputs = model(**inputs)\n",
        "            # model outputs are always tuple in transformers (see doc)\n",
        "            loss = outputs[0]\n",
        "\n",
        "            if args.gradient_accumulation_steps > 1:\n",
        "                loss = loss / args.gradient_accumulation_steps\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            tr_loss += loss.item()\n",
        "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
        "\n",
        "                optimizer.step()\n",
        "                scheduler.step()  # Update learning rate schedule\n",
        "                model.zero_grad()\n",
        "                global_step += 1\n",
        "\n",
        "                # Log metrics\n",
        "                if args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
        "                    # Only evaluate when single GPU otherwise metrics may not average well\n",
        "                    if args.evaluate_during_training:\n",
        "                        results = evaluation(args, model, tokenizer, global_step=global_step)\n",
        "                        for key in sorted(results.keys()):\n",
        "                            logger.info(\"  %s = %s\", key, str(results[key]))\n",
        "\n",
        "                    logging_loss = tr_loss\n",
        "\n",
        "                # Save model checkpoint\n",
        "                if args.save_steps > 0 and global_step % args.save_steps == 0:\n",
        "                    output_dir = os.path.join(args.output_dir, \"checkpoint-{}\".format(global_step))\n",
        "                    if not os.path.exists(output_dir):\n",
        "                        os.makedirs(output_dir)\n",
        "                    # Take care of distributed/parallel training\n",
        "                    model_to_save = model.module if hasattr(model, \"module\") else model\n",
        "                    model_to_save.save_pretrained(output_dir)\n",
        "                    tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "                    torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n",
        "                    logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
        "\n",
        "                    if args.save_optimizer:\n",
        "                        torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n",
        "                        torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n",
        "                        logger.info(\"Saving optimizer and scheduler states to %s\", output_dir)\n",
        "\n",
        "            if args.max_steps > 0 and global_step > args.max_steps:\n",
        "                break\n",
        "\n",
        "        mb.write(\"Epoch {} done\".format(epoch+1))\n",
        "\n",
        "        if args.max_steps > 0 and global_step > args.max_steps:\n",
        "            break\n",
        "\n",
        "    return global_step, tr_loss / global_step\n",
        "\n",
        "\n",
        "def evaluation(args, model, tokenizer, global_step=None):\n",
        "    dataset, examples, features = load_and_cache_examples(args, tokenizer, evaluate=True, output_examples=True)\n",
        "\n",
        "    if not os.path.exists(args.output_dir):\n",
        "        os.makedirs(args.output_dir)\n",
        "\n",
        "    # Note that DistributedSampler samples randomly\n",
        "    eval_sampler = SequentialSampler(dataset)\n",
        "    eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
        "\n",
        "    # Eval!\n",
        "    logger.info(\"***** Running evaluation {} *****\".format(global_step))\n",
        "    logger.info(\"  Num examples = %d\", len(dataset))\n",
        "    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
        "\n",
        "    all_results = []\n",
        "    start_time = timeit.default_timer()\n",
        "\n",
        "    for batch in progress_bar(eval_dataloader):\n",
        "        model.eval()\n",
        "        batch = tuple(t.to(args.device) for t in batch)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            inputs = {\n",
        "                \"input_ids\": batch[0],\n",
        "                \"attention_mask\": batch[1],\n",
        "                \"token_type_ids\": batch[2],\n",
        "            }\n",
        "\n",
        "            if args.model_type in [\"xlm\", \"roberta\", \"distilbert\", \"distilkobert\", \"xlm-roberta\"]:\n",
        "                del inputs[\"token_type_ids\"]\n",
        "\n",
        "            example_indices = batch[3]\n",
        "\n",
        "            outputs = model(**inputs)\n",
        "            \n",
        "            start_logits = outputs[0] # bs 512\n",
        "            end_logits = outputs[1] # torch.Tensor\n",
        "            \n",
        "            \n",
        "            # print(len(start_logits)\n",
        "            # print(len(start_logits[0]))\n",
        "            \n",
        "            \n",
        "            \n",
        "            \n",
        "            \n",
        "        # print(len(end_logits))\n",
        "        # print(len(end_logits[0]))\n",
        "        \n",
        "\n",
        "        for i, example_index in enumerate(example_indices):\n",
        "            start_logit = start_logits[i].cpu().tolist()\n",
        "            end_logit = end_logits[i].cpu().tolist()\n",
        "            # start_logits = start_logits[i]\n",
        "            # end_logits = end_logits[i]\n",
        "            # start_logits = start_logits[i]\n",
        "            # end_logits = end_logits[i]\n",
        "            eval_feature = features[example_index.item()]\n",
        "            unique_id = int(eval_feature.unique_id)\n",
        "            result = SquadResult(unique_id, start_logit, end_logit)\n",
        "\n",
        "            all_results.append(result)\n",
        "\n",
        "    evalTime = timeit.default_timer() - start_time\n",
        "    logger.info(\"  Evaluation done in total %f secs (%f sec per example)\", evalTime, evalTime / len(dataset))\n",
        "\n",
        "    # Compute predictions\n",
        "    output_prediction_file = os.path.join(args.output_dir, \"predictions_{}.json\".format(global_step))\n",
        "    output_nbest_file = os.path.join(args.output_dir, \"nbest_predictions_{}.json\".format(global_step))\n",
        "\n",
        "    if args.version_2_with_negative:\n",
        "        output_null_log_odds_file = os.path.join(args.output_dir, \"null_odds_{}.json\".format(global_step))\n",
        "    else:\n",
        "        output_null_log_odds_file = None\n",
        "\n",
        "    predictions = compute_predictions_logits(\n",
        "        examples,\n",
        "        features,\n",
        "        all_results,\n",
        "        args.n_best_size,\n",
        "        args.max_answer_length,\n",
        "        args.do_lower_case,\n",
        "        output_prediction_file,\n",
        "        output_nbest_file,\n",
        "        output_null_log_odds_file,\n",
        "        args.verbose_logging,\n",
        "        args.version_2_with_negative,\n",
        "        args.null_score_diff_threshold,\n",
        "        tokenizer,\n",
        "    )\n",
        "\n",
        "    # Compute the F1 and exact scores.\n",
        "    results = squad_evaluate(examples, predictions)\n",
        "    # Write the result\n",
        "    # Write the evaluation result on file\n",
        "    output_dir = os.path.join(args.output_dir, 'eval')\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    output_eval_file = os.path.join(output_dir, \"eval_result_{}_{}.txt\".format(list(filter(None, args.model_name_or_path.split(\"/\"))).pop(),\n",
        "                                                                               global_step))\n",
        "\n",
        "    with open(output_eval_file, \"w\", encoding='utf-8') as f:\n",
        "        official_eval_results = eval_during_train(args, step=global_step)\n",
        "        results.update(official_eval_results)\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def load_and_cache_examples(args, tokenizer, evaluate=False, output_examples=False):\n",
        "    # Load data features from cache or dataset file\n",
        "    input_dir = args.data_dir if args.data_dir else \".\"\n",
        "    cached_features_file = os.path.join(\n",
        "        input_dir,\n",
        "        \"cached_{}_{}_{}\".format(\n",
        "            \"dev\" if evaluate else \"train\",\n",
        "            list(filter(None, args.model_name_or_path.split(\"/\"))).pop(),\n",
        "            str(args.max_seq_length),\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    # Init features and dataset from cache if it exists\n",
        "    if os.path.exists(cached_features_file):\n",
        "        logger.info(\"Loading features from cached file %s\", cached_features_file)\n",
        "        features_and_dataset = torch.load(cached_features_file)\n",
        "        features, dataset, examples = (\n",
        "            features_and_dataset[\"features\"],\n",
        "            features_and_dataset[\"dataset\"],\n",
        "            features_and_dataset[\"examples\"],\n",
        "        )\n",
        "    else:\n",
        "        logger.info(\"Creating features from dataset file at %s\", input_dir)\n",
        "\n",
        "        if not args.data_dir and ((evaluate and not args.predict_file) or (not evaluate and not args.train_file)):\n",
        "            try:\n",
        "                import tensorflow_datasets as tfds\n",
        "            except ImportError:\n",
        "                raise ImportError(\"If not data_dir is specified, tensorflow_datasets needs to be installed.\")\n",
        "\n",
        "            if args.version_2_with_negative:\n",
        "                logger.warn(\"tensorflow_datasets does not handle version 2 of SQuAD.\")\n",
        "\n",
        "            tfds_examples = tfds.load(\"squad\")\n",
        "            examples = SquadV1Processor().get_examples_from_dataset(tfds_examples, evaluate=evaluate)\n",
        "        else:\n",
        "            processor = SquadV2Processor() if args.version_2_with_negative else SquadV1Processor()\n",
        "            if evaluate:\n",
        "                examples = processor.get_dev_examples(os.path.join(args.data_dir),\n",
        "                                                      filename=args.predict_file)\n",
        "            else:\n",
        "                examples = processor.get_train_examples(os.path.join(args.data_dir),\n",
        "                                                        filename=args.train_file)\n",
        "\n",
        "        features, dataset = squad_convert_examples_to_features(\n",
        "            examples=examples,\n",
        "            tokenizer=tokenizer,\n",
        "            max_seq_length=args.max_seq_length,\n",
        "            doc_stride=args.doc_stride,\n",
        "            max_query_length=args.max_query_length,\n",
        "            is_training=not evaluate,\n",
        "            return_dataset=\"pt\",\n",
        "            threads=args.threads,\n",
        "        )\n",
        "\n",
        "        logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
        "        torch.save({\"features\": features, \"dataset\": dataset, \"examples\": examples}, cached_features_file)\n",
        "\n",
        "    if output_examples:\n",
        "        return dataset, examples, features\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def main(args):\n",
        "    # Read from config file and make args\n",
        "    logger.info(\"Training/evaluation parameters {}\".format(args))\n",
        "\n",
        "    args.output_dir = os.path.join(args.output_dir)\n",
        "\n",
        "    if args.doc_stride >= args.max_seq_length - args.max_query_length:\n",
        "        logger.warning(\n",
        "            \"WARNING - You've set a doc stride which may be superior to the document length in some \"\n",
        "            \"examples. This could result in errors when building features from the examples. Please reduce the doc \"\n",
        "            \"stride or increase the maximum length to ensure the features are correctly built.\"\n",
        "        )\n",
        "\n",
        "    init_logger()\n",
        "    set_seed(args)\n",
        "\n",
        "    logging.getLogger(\"transformers.data.metrics.squad_metrics\").setLevel(logging.WARN)  # Reduce model loading logs\n",
        "\n",
        "    # Load pretrained model and tokenizer\n",
        "    config = CONFIG_CLASSES[args.model_type].from_pretrained(\n",
        "        args.model_name_or_path,\n",
        "    )\n",
        "    tokenizer = TOKENIZER_CLASSES[args.model_type].from_pretrained(\n",
        "        args.model_name_or_path,\n",
        "        do_lower_case=args.do_lower_case,\n",
        "    )\n",
        "    model = MODEL_FOR_QUESTION_ANSWERING[args.model_type].from_pretrained(\n",
        "        args.model_name_or_path,\n",
        "        config=config,\n",
        "    )\n",
        "    # GPU or CPU\n",
        "    args.device = \"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\"\n",
        "    model.to(args.device)\n",
        "\n",
        "    logger.info(\"Training/evaluation parameters %s\", args)\n",
        "\n",
        "    # Training\n",
        "    if args.do_train:\n",
        "        train_dataset = load_and_cache_examples(args, tokenizer, evaluate=False, output_examples=False)\n",
        "        global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n",
        "        logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n",
        "\n",
        "    # Evaluation - we can ask to evaluate all the checkpoints (sub-directories) in a directory\n",
        "    results = {}\n",
        "    if args.do_eval:\n",
        "        checkpoints = list(\n",
        "            os.path.dirname(c)\n",
        "            for c in sorted(glob.glob(args.output_dir + \"/**/\" + \"pytorch_model.bin\", recursive=True))\n",
        "        )\n",
        "        if not args.eval_all_checkpoints:\n",
        "            checkpoints = checkpoints[-1:]\n",
        "        else:\n",
        "            logging.getLogger(\"transformers.configuration_utils\").setLevel(logging.WARN)  # Reduce model loading logs\n",
        "            logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.WARN)  # Reduce model loading logs\n",
        "\n",
        "        logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n",
        "\n",
        "        for checkpoint in checkpoints:\n",
        "            # Reload the model\n",
        "            global_step = checkpoint.split(\"-\")[-1]\n",
        "            model = MODEL_FOR_QUESTION_ANSWERING[args.model_type].from_pretrained(checkpoint)\n",
        "            model.to(args.device)\n",
        "            result = evaluation(args, model, tokenizer, global_step=global_step)\n",
        "            result = dict((k + (\"_{}\".format(global_step) if global_step else \"\"), v) for k, v in result.items())\n",
        "            results.update(result)\n",
        "\n",
        "        output_eval_file = os.path.join(args.output_dir, \"eval_results.txt\")\n",
        "        with open(output_eval_file, \"w\") as f_w:\n",
        "            for key in sorted(results.keys()):\n",
        "                f_w.write(\"{} = {}\\n\".format(key, str(results[key])))              \n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwRaWZ7LKjjh"
      },
      "source": [
        "if __name__ == \"__main__\" :\n",
        "\n",
        "    \n",
        "    # https://github.com/monologg/KoELECTRA/blob/master/finetune/config/korquad/koelectra-small-v3.json \n",
        "    \n",
        "    hyper_para={\n",
        "    \"task\": \"korquad\",\n",
        "    \"data_dir\": data_dir,\n",
        "    \"ckpt_dir\": \"ckpt\",\n",
        "    \"train_file\": \"KorQuAD_v1.0_train.json\",\n",
        "    \"predict_file\": \"KorQuAD_v1.0_dev.json\",\n",
        "    \"threads\": 4,\n",
        "    \"version_2_with_negative\": False,\n",
        "    \"null_score_diff_threshold\": 0.0,\n",
        "    \"max_seq_length\": 512,\n",
        "    \"doc_stride\": 128,\n",
        "    \"max_query_length\": 64,\n",
        "    \"max_answer_length\": 30,\n",
        "    \"n_best_size\": 20,\n",
        "    \"verbose_logging\": True,\n",
        "    \"overwrite_output_dir\": True,\n",
        "    \"evaluate_during_training\": True,\n",
        "    \"eval_all_checkpoints\": True,\n",
        "    \"save_optimizer\": False,\n",
        "    \"do_lower_case\": False,\n",
        "    \"do_train\": True,\n",
        "    \"do_eval\": True,\n",
        "    \"num_train_epochs\": 10,\n",
        "    \"weight_decay\": 0.0,\n",
        "    \"gradient_accumulation_steps\": 1,\n",
        "    \"adam_epsilon\": 1e-8,\n",
        "    \"warmup_proportion\": 0,\n",
        "    \"max_steps\": -1,\n",
        "    \"max_grad_norm\": 1.0,\n",
        "    \"no_cuda\": False,\n",
        "    \"model_type\": \"koelectra-small-v3\",\n",
        "    \"model_name_or_path\": \"monologg/koelectra-small-v3-discriminator\",\n",
        "    \"output_dir\": output_dir,\n",
        "    \"seed\": 42,\n",
        "    \"train_batch_size\": 32, \n",
        "    \"eval_batch_size\": 64,\n",
        "    \"logging_steps\": 3000,\n",
        "    \"save_steps\": 3000,\n",
        "    \"learning_rate\": 5e-5\n",
        "    }\n",
        "\n",
        "    main(AttrDict(hyper_para))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y996pTI-ilaW"
      },
      "source": [
        "test_file = '/content/gdrive/My Drive/Colab Notebooks/DFC615/test.json'\n",
        "test_json = json.load(open(test_file,'r', encoding='utf-8'))\n",
        "\n",
        "for sample in test_json['data']:\n",
        "      context = sample['context']\n",
        "      question = sample['question']\n",
        "\n",
        "import os\n",
        "\n",
        "pretrained_checkpoint = \"/content/gdrive/MyDrive/Colab Notebooks/DFC615/output/checkpoint-18000\"\n",
        "tokenizer = ElectraTokenizer.from_pretrained(pretrained_checkpoint)\n",
        "model = ElectraForQuestionAnswering.from_pretrained(pretrained_checkpoint)\n",
        "prediction_qa = pipeline(\"question-answering\", tokenizer=tokenizer, model=model)     \n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4_x73AvmDxO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "227395e0-2a0e-4ba0-94bc-48df57b8ab76"
      },
      "source": [
        "my_answer = {}\n",
        "my_answer['id']=[]\n",
        "my_answer['prediction_text']=[]\n",
        "\n",
        "for idx, sample in enumerate(test_json['data']):\n",
        "    context = sample['context']\n",
        "    question = sample['question']\n",
        "\n",
        "    answer_dict = prediction_qa({'question':question,'context': context})\n",
        "    predict_text = normalize_answer(answer_dict[\"answer\"]) # 실행중 hanging 되는 경우가 있음. 런타임 초기화후 재실행으로 해결\n",
        "    \n",
        "    my_answer['id'].append(str(idx + 1))\n",
        "    my_answer['prediction_text'].append(predict_text)\n",
        "\n",
        "df = pd.DataFrame(my_answer)\n",
        "df.to_csv(data_dir + '/my_answer3.csv', index=False, encoding='utf-8')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "치안관리처벌법에 [('치안관리처벌법', 'N'), ('에', 'J')] 치안관리처벌법\n",
            "2013년 [('2013년', 'N')] 2013년\n",
            "빛은 [('빛', 'N'), ('은', 'J')] 빛\n",
            "타이베리움을 [('타이베리움', 'N'), ('을', 'J')] 타이베리움\n",
            "11월 3일 [('11월', 'N'), ('3일', 'N')] 11월 3일\n",
            "대구 경북지역에서 [('대구', 'N'), ('경북지역', 'N'), ('에서', 'J')] 대구 경북지역\n",
            "10로 [('10', 'N'), ('로', 'J')] 10\n",
            "save me [('save', 'F'), ('me', 'F')] save me\n",
            "제3차 하리코프 공방전에 [('제3차', 'N'), ('하리코프', 'N'), ('공방전', 'N'), ('에', 'J')] 제3차 하리코프 공방전\n",
            "동대문아파트는 [('동대문아파트', 'N'), ('는', 'J')] 동대문아파트\n",
            "사라예보에 [('사라예보', 'N'), ('에', 'J')] 사라예보\n",
            "경영자 [('경영자', 'N')] 경영자\n",
            "자신의 개인 활동에 [('자신', 'N'), ('의', 'J'), ('개', 'N'), ('이', 'J'), ('ㄴ', 'E'), ('활동', 'N'), ('에', 'J')] 자신의 개인 활동\n",
            "다케조에 신이치로에겐 [('다케조에', 'N'), ('신', 'X'), ('이치로', 'N'), ('에겐', 'J')] 다케조에 신이치로\n",
            "빛의 제국은 [('빛', 'N'), ('의', 'J'), ('제국', 'N'), ('은', 'J')] 빛의 제국\n",
            "섭외민사관계 법률적용법은 [('섭외민사관계', 'N'), ('법률적용법', 'N'), ('은', 'J')] 섭외민사관계 법률적용법\n",
            "김대중 김종필 [('김대중', 'N'), ('김종필', 'N')] 김대중 김종필\n",
            "쥐트슐라이페라는 [('쥐트슐라이페', 'N'), ('라는', 'J')] 쥐트슐라이페\n",
            "이완과 [('이완', 'N'), ('과', 'J')] 이완\n",
            "22조 9000억원으로 [('22조', 'N'), ('9000억원', 'N'), ('으로', 'J')] 22조 9000억원\n",
            "600000장이 [('600000장', 'N'), ('이', 'J')] 600000장\n",
            "음악적 예술 형태이다 [('음악적', 'N'), ('예술', 'N'), ('형태', 'N'), ('이', 'J'), ('다', 'E')] 음악적 예술 형태\n",
            "시카고 블루스라 [('시카', 'N'), ('이', 'J'), ('고', 'E'), ('블루스라', 'N')] 시카고 블루스라\n",
            "세계일보에서는 [('세계일보', 'N'), ('에서는', 'J')] 세계일보\n",
            "블록체인 [('블록체인', 'N')] 블록체인\n",
            "지난해 4월에 [('지난해', 'N'), ('4월', 'N'), ('에', 'J')] 지난해 4월\n",
            "부산에서 [('부산', 'N'), ('에서', 'J')] 부산\n",
            "30일간 [('30일', 'N'), ('간', 'X')] 30일간\n",
            "마스크를 [('마스크', 'N'), ('를', 'J')] 마스크\n",
            "샤이크 미스킨에서는 [('샤이크', 'N'), ('미스킨', 'N'), ('에서는', 'J')] 샤이크 미스킨\n",
            "15년까지만 [('15년', 'N'), ('까지만', 'J')] 15년\n",
            "sbs에 [('sbs', 'F'), ('에', 'J')] sbs\n",
            "1992년 [('1992년', 'N')] 1992년\n",
            "장진 [('장진', 'N')] 장진\n",
            "요르단 강 [('요르단', 'N'), ('강', 'N')] 요르단 강\n",
            "서울올림픽주경기장에서 [('서울올림픽주경기장', 'N'), ('에서', 'J')] 서울올림픽주경기장\n",
            "이두황 [('이두황', 'N')] 이두황\n",
            "업의 성질의 [('업', 'N'), ('의', 'J'), ('성질', 'N'), ('의', 'J')] 업의 성질\n",
            "정수기 cf가 [('정수', 'N'), ('이', 'J'), ('기', 'E'), ('cf', 'F'), ('가', 'J')] 정수기 cf\n",
            "차범근 축구교실을 [('차범근', 'N'), ('축구교실', 'N'), ('을', 'J')] 차범근 축구교실\n",
            "장미아파트로 [('장미아파트', 'N'), ('로', 'J')] 장미아파트\n",
            "1월 2일 [('1월', 'N'), ('2일', 'N')] 1월 2일\n",
            "300만 원 [('300', 'N'), ('만', 'J'), ('원', 'I')] 300만 원\n",
            "이회창은 [('이회창', 'N'), ('은', 'J')] 이회창\n",
            "진상은 이렇다라는 [('진상', 'N'), ('은', 'J'), ('이렇', 'P'), ('다라는', 'E')] 진상은 이렇\n",
            "평민당사를 [('평민당사', 'N'), ('를', 'J')] 평민당사\n",
            "이븐 알나딤 [('이븐', 'N'), ('알나딤', 'N')] 이븐 알나딤\n",
            "문헌이라는 [('문헌', 'N'), ('이라는', 'J')] 문헌\n",
            "2007년 12월 7일 [('2007년', 'N'), ('12월', 'N'), ('7일', 'N')] 2007년 12월 7일\n",
            "윤상 콘서트play [('윤상', 'N'), ('콘서트play', 'N')] 윤상 콘서트play\n",
            "하프연주에 [('하프연주', 'N'), ('에', 'J')] 하프연주\n",
            "대포동 2호를 [('대포동', 'N'), ('2호', 'N'), ('를', 'J')] 대포동 2호\n",
            "인천을 [('인천', 'N'), ('을', 'J')] 인천\n",
            "워싱턴포스트 신문은 [('워싱턴포스트', 'N'), ('신문', 'N'), ('은', 'J')] 워싱턴포스트 신문\n",
            "2012년 7월 5일 [('2012년', 'N'), ('7월', 'N'), ('5일', 'N')] 2012년 7월 5일\n",
            "화합적취 즉 화합하여 쌓인다는 뜻이다 [('화합적취', 'N'), ('즉', 'M'), ('화합', 'N'), ('하', 'X'), ('어', 'E'), ('쌓이', 'P'), ('ㄴ다는', 'E'), ('뜻', 'N'), ('이', 'J'), ('다', 'E')] 화합적취 즉 화합하여 쌓인다는 뜻\n",
            "김홍집을 [('김홍집', 'N'), ('을', 'J')] 김홍집\n",
            "환경권이 [('환경권', 'N'), ('이', 'J')] 환경권\n",
            "우리 집에 왜 왔니에 [('우리', 'N'), ('집', 'N'), ('에', 'J'), ('왜', 'M'), ('왔니', 'N'), ('에', 'J')] 우리 집에 왜 왔니\n",
            "seethrough hmd를 [('seethrough', 'F'), ('hmd', 'F'), ('를', 'J')] seethrough hmd\n",
            "도둑들에서 [('도둑들', 'N'), ('에서', 'J')] 도둑들\n",
            "알렉산드르 1세는 [('알렉산드르', 'N'), ('1세', 'N'), ('는', 'J')] 알렉산드르 1세\n",
            "련정희 [('련정희', 'N')] 련정희\n",
            "1천100억 달러를 [('1천100억', 'N'), ('달러', 'N'), ('를', 'J')] 1천100억 달러\n",
            "승해 [('승하', 'P'), ('어', 'E')] 승해\n",
            "내부 기생충이 [('내부', 'N'), ('기생충', 'N'), ('이', 'J')] 내부 기생충\n",
            "점프 블루스 [('점프', 'N'), ('블루스', 'N')] 점프 블루스\n",
            "스크린을 [('스크린', 'N'), ('을', 'J')] 스크린\n",
            "김구는 [('김구', 'N'), ('는', 'J')] 김구\n",
            "약 100년이 [('약', 'N'), ('100년', 'N'), ('이', 'J')] 약 100년\n",
            "암호화폐는 [('암호화폐', 'N'), ('는', 'J')] 암호화폐\n",
            "정도전에게 [('정도전', 'N'), ('에게', 'J')] 정도전\n",
            "대륙 철학이라는 [('대륙', 'N'), ('철학', 'N'), ('이라는', 'J')] 대륙 철학\n",
            "상수도와 하수도도 [('상수도', 'N'), ('와', 'J'), ('하수도', 'N'), ('도', 'J')] 상수도와 하수도\n",
            "초등학교 교사로서 [('초등학교', 'N'), ('교사', 'N'), ('로서', 'J')] 초등학교 교사\n",
            "하페즈 알아사드는 [('하페즈', 'N'), ('알아사드', 'N'), ('는', 'J')] 하페즈 알아사드\n",
            "아덴만 여명작전에 [('아덴만', 'N'), ('여명작전', 'N'), ('에', 'J')] 아덴만 여명작전\n",
            "김영대 [('김영대', 'N')] 김영대\n",
            "1949년 [('1949년', 'N')] 1949년\n",
            "개정 [('개정', 'N')] 개정\n",
            "향토예비군법 폐지안을 [('향토예비군법', 'N'), ('폐지안', 'N'), ('을', 'J')] 향토예비군법 폐지안\n",
            "외젠 드 보아르네에게 [('외젠', 'N'), ('드', 'N'), ('보아르', 'N'), ('네', 'X'), ('에게', 'J')] 외젠 드 보아르네\n",
            "높이에 [('높이', 'N'), ('에', 'J')] 높이\n",
            "사 [('사', 'N')] 사\n",
            "김대중은 [('김대중', 'N'), ('은', 'J')] 김대중\n",
            "암흑국가라면서 [('암흑국가', 'N'), ('라면서', 'J')] 암흑국가\n",
            "북한 잠수함 침투사건 [('북한', 'N'), ('잠수함', 'N'), ('침투사건', 'N')] 북한 잠수함 침투사건\n",
            "해금 정책으로 [('해금', 'N'), ('정책', 'N'), ('으로', 'J')] 해금 정책\n",
            "후쿠시마 원전사고이후 [('후쿠시마', 'N'), ('원전사고이후', 'N')] 후쿠시마 원전사고이후\n",
            "제국의 아이들의 [('제국', 'N'), ('의', 'J'), ('아이들', 'N'), ('의', 'J')] 제국의 아이들\n",
            "김옥균은 [('김옥균', 'N'), ('은', 'J')] 김옥균\n",
            "미국 육군 부참모 총장과 [('미국', 'N'), ('육군', 'N'), ('부참모', 'N'), ('총장', 'N'), ('과', 'J')] 미국 육군 부참모 총장\n",
            "동진사로 [('동진사', 'N'), ('로', 'J')] 동진사\n",
            "호남출신인 [('호남출신', 'N'), ('이', 'J'), ('ㄴ', 'E')] 호남출신인\n",
            "10명을 [('10명', 'N'), ('을', 'J')] 10명\n",
            "7월 8일 [('7월', 'N'), ('8일', 'N')] 7월 8일\n",
            "죄형법정주의에 [('죄형법정주의', 'N'), ('에', 'J')] 죄형법정주의\n",
            "일본 닌텐도 스페이스 월드 엑스포에서 [('일본', 'N'), ('닌텐', 'N'), ('도', 'J'), ('스페이스', 'N'), ('월드', 'N'), ('엑스포', 'N'), ('에서', 'J')] 일본 닌텐도 스페이스 월드 엑스포\n",
            "24명의 [('24명', 'N'), ('의', 'J')] 24명\n",
            "3가지로 [('3가지', 'N'), ('로', 'J')] 3가지\n",
            "abc방송사에서 [('abc', 'F'), ('방송사', 'N'), ('에서', 'J')] abc방송사\n",
            "clichè를 [('clichè', 'N'), ('를', 'J')] clichè\n",
            "2014년 [('2014년', 'N')] 2014년\n",
            "로베르타 캐슬린 팍스가 [('로베르타', 'N'), ('캐슬린', 'N'), ('팍스', 'N'), ('가', 'J')] 로베르타 캐슬린 팍스\n",
            "공무원 급여를 [('공무원', 'N'), ('급여', 'N'), ('를', 'J')] 공무원 급여\n",
            "국민신당을 [('국민신당', 'N'), ('을', 'J')] 국민신당\n",
            "1958년 [('1958년', 'N')] 1958년\n",
            "전국 천하영웅의 시대에 [('전국', 'N'), ('천하영웅', 'N'), ('의', 'J'), ('시대', 'N'), ('에', 'J')] 전국 천하영웅의 시대\n",
            "체크메이트 [('체크메이트', 'N')] 체크메이트\n",
            "블루스 페스티벌은 [('블루스', 'N'), ('페스티벌', 'N'), ('은', 'J')] 블루스 페스티벌\n",
            "구약학 [('구약학', 'N')] 구약학\n",
            "빌헬름 마르크스를 [('빌헬름', 'N'), ('마르크스', 'N'), ('를', 'J')] 빌헬름 마르크스\n",
            "40명은 [('40명', 'N'), ('은', 'J')] 40명\n",
            "할아버지의 [('할아버지', 'N'), ('의', 'J')] 할아버지\n",
            "노무현 정권의 [('노무현', 'N'), ('정권', 'N'), ('의', 'J')] 노무현 정권\n",
            "김용옥은 [('김용옥', 'N'), ('은', 'J')] 김용옥\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}